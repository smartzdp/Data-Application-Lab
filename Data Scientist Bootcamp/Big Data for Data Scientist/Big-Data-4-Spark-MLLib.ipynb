{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data 4 - Spark MLLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ssh] Login to montana.dataapplab.com...\n",
      "[ssh] host=montana.dataapplab.com hostname=montana.dataapplab.com other_conf={'user': 'dcdsdepeizhang', 'port': 49233, 'keyfile': ['/home/dz3vg/.ssh/id_rsa'], 'load_system_ssh_config': False, 'missing_host_policy': <paramiko.client.WarningPolicy object at 0x7fdc54aa4a10>}\n",
      "[ssh] forwarding local agent\n",
      "[ssh] Successfully logged in.\n"
     ]
    }
   ],
   "source": [
    "%login montana.dataapplab.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ssh] host = montana.dataapplab.com, cwd = /home/dcdsdepeizhang\n",
      "/home/dcdsdepeizhang\n"
     ]
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ssh] host = montana.dataapplab.com, cwd = /home/dcdsdepeizhang\n",
      "data\n",
      "demo\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ssh] host = montana.dataapplab.com, cwd = /home/dcdsdepeizhang\n",
      "[ssh] new cwd: /home/dcdsdepeizhang/demo\n"
     ]
    }
   ],
   "source": [
    "cd demo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ssh] host = montana.dataapplab.com, cwd = /home/dcdsdepeizhang/demo\n",
      "20/07/30 00:12:06 INFO fs.TrashPolicyDefault: Moved: 'hdfs://m1.mt.dataapplab.com:8020/user/dcdsdepeizhang/data/iris.csv' to trash at: hdfs://m1.mt.dataapplab.com:8020/user/dcdsdepeizhang/.Trash/Current/user/dcdsdepeizhang/data/iris.csv\n",
      "20/07/30 00:12:09 INFO fs.TrashPolicyDefault: Moved: 'hdfs://m1.mt.dataapplab.com:8020/user/dcdsdepeizhang/models/iris-NaiveBayesModel' to trash at: hdfs://m1.mt.dataapplab.com:8020/user/dcdsdepeizhang/.Trash/Current/user/dcdsdepeizhang/models/iris-NaiveBayesModel1596093129058\n"
     ]
    }
   ],
   "source": [
    "rm /home/dcdsdepeizhang/data/iris.csv\n",
    "hdfs dfs -rm hdfs:///user/dcdsdepeizhang/data/iris.csv\n",
    "hdfs dfs -rm -r -f hdfs:///user/dcdsdepeizhang/models/iris-NaiveBayesModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ssh] host = montana.dataapplab.com, cwd = /home/dcdsdepeizhang/demo\n"
     ]
    }
   ],
   "source": [
    "hdfs dfs -get hdfs:///user/jason/data/iris.csv /home/dcdsdepeizhang/data/iris.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ssh] host = montana.dataapplab.com, cwd = /home/dcdsdepeizhang/demo\n",
      "/home/dcdsdepeizhang/data/iris.csv\n"
     ]
    }
   ],
   "source": [
    "ls /home/dcdsdepeizhang/data/iris.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ssh] host = montana.dataapplab.com, cwd = /home/dcdsdepeizhang/demo\n"
     ]
    }
   ],
   "source": [
    "hdfs dfs -put /home/dcdsdepeizhang/data/iris.csv hdfs:///user/dcdsdepeizhang/data/iris.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ssh] host = montana.dataapplab.com, cwd = /home/dcdsdepeizhang/demo\n",
      "-rw-r--r--   3 dcdsdepeizhang student       5107 2020-07-30 00:12 hdfs:///user/dcdsdepeizhang/data/iris.csv\n"
     ]
    }
   ],
   "source": [
    "hdfs dfs -ls hdfs:///user/dcdsdepeizhang/data/iris.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ssh] host = montana.dataapplab.com, cwd = /home/dcdsdepeizhang/demo\n"
     ]
    }
   ],
   "source": [
    "cat << EOF > spark-mllib.py\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import NaiveBayes, NaiveBayesModel, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "\n",
    "struct = StructType([ \n",
    "    StructField('Id',            IntegerType(), True), \n",
    "    StructField('SepalLengthCm', DoubleType(),  True), \n",
    "    StructField('SepalWidthCm',  DoubleType(),  True), \n",
    "    StructField('PetalLengthCm', DoubleType(),  True), \n",
    "    StructField('PetalWidthCm',  DoubleType(),  True), \n",
    "    StructField('Species',       StringType(),  True)\n",
    "])\n",
    "\n",
    "print \"\\nCreate DataFrame\\n\"\n",
    "df_iris = spark.read.csv('hdfs:///user/dcdsdepeizhang/data/iris.csv', header=True, schema=struct)\n",
    "df_iris.printSchema()\n",
    "df_iris.describe().show()\n",
    "df_iris.show(5)\n",
    "\n",
    "print \"\\nVectorize Features & Label Target\\n\"\n",
    "vecAssembler = VectorAssembler(inputCols=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"], outputCol=\"features\")\n",
    "df_features = vecAssembler.transform(df_iris)\n",
    "stringIndexer = StringIndexer(inputCol=\"Species\", outputCol=\"label\")\n",
    "df_features_label = stringIndexer.fit(df_features).transform(df_features)\n",
    "df_features_label.show(5)\n",
    "# df_features_label.select('label').distinct().show()\n",
    "\n",
    "print \"\\nNaive Bayes Model\\n\"\n",
    "df_train, df_test = df_features_label.randomSplit([.7, .3])\n",
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\")\n",
    "nb_model = nb.fit(df_train)\n",
    "print nb_model\n",
    "\n",
    "print \"\\nTest Model on a Trial Vector\\n\"\n",
    "X_trial = sc.parallelize([Row(features=Vectors.dense([5.1, 3.5, 1.4, 0.2]))]).toDF()\n",
    "y_trial = nb_model.transform(X_trial)\n",
    "y_trial.show()\n",
    "\n",
    "print \"\\nPredict on Test Set\\n\"\n",
    "df_predicted = nb_model.transform(df_test.select('features', 'label'))\n",
    "df_predicted.show(5)\n",
    "\n",
    "print \"\\nEvaluate Model\\n\"\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "print evaluator.evaluate(df_predicted)\n",
    "\n",
    "print \"\\nExport Model & Import Model\\n\"\n",
    "output_dir = \"hdfs:///user/dcdsdepeizhang/models/iris-NaiveBayesModel\"\n",
    "nb_model.save(output_dir)\n",
    "nb_model = NaiveBayesModel.load(output_dir)\n",
    "y_trial = nb_model.transform(X_trial)\n",
    "y_trial.show()\n",
    "\n",
    "print \"\\nDecision Tree Model\\n\"\n",
    "dt = DecisionTreeClassifier(maxDepth=5, featuresCol=\"features\", labelCol=\"label\")\n",
    "dt_model = dt.fit(df_train)\n",
    "print dt_model.toDebugString\n",
    "\n",
    "print \"\\nTest Model on a Trial Vector\\n\"\n",
    "y_trial = dt_model.transform(X_trial)\n",
    "y_trial.show()\n",
    "\n",
    "print \"\\nPredict on Test Set\\n\"\n",
    "df_predicted = dt_model.transform(df_test.select('features', 'label'))\n",
    "df_predicted.show(5)\n",
    "\n",
    "print \"\\nEvaluate Model\\n\"\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "print evaluator.evaluate(df_predicted)\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ssh] host = montana.dataapplab.com, cwd = /home/dcdsdepeizhang/demo\n",
      "SPARK_MAJOR_VERSION is set to 2, using Spark2\n",
      "20/07/30 00:12:28 INFO SparkContext: Running Spark version 2.0.0.2.5.3.0-37\n",
      "20/07/30 00:12:28 INFO SecurityManager: Changing view acls to: dcdsdepeizhang\n",
      "20/07/30 00:12:28 INFO SecurityManager: Changing modify acls to: dcdsdepeizhang\n",
      "20/07/30 00:12:28 INFO SecurityManager: Changing view acls groups to: \n",
      "20/07/30 00:12:28 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/07/30 00:12:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dcdsdepeizhang); groups with view permissions: Set(); users  with modify permissions: Set(dcdsdepeizhang); groups with modify permissions: Set()\n",
      "20/07/30 00:12:29 INFO Utils: Successfully started service 'sparkDriver' on port 44974.\n",
      "20/07/30 00:12:29 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/07/30 00:12:29 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/07/30 00:12:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1ed1e485-5ba0-4f8c-a56c-a79778738777\n",
      "20/07/30 00:12:29 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "20/07/30 00:12:29 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/07/30 00:12:29 INFO log: Logging initialized @2864ms\n",
      "20/07/30 00:12:29 INFO Server: jetty-9.2.z-SNAPSHOT\n",
      "20/07/30 00:12:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@12c15a9f{/jobs,null,AVAILABLE}\n",
      "20/07/30 00:12:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1da7dfaf{/jobs/json,null,AVAILABLE}\n",
      "20/07/30 00:12:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@17571bff{/jobs/job,null,AVAILABLE}\n",
      "20/07/30 00:12:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2d482dd8{/jobs/job/json,null,AVAILABLE}\n",
      "20/07/30 00:12:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@35458f4c{/stages,null,AVAILABLE}\n",
      "20/07/30 00:12:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@16bc64ec{/stages/json,null,AVAILABLE}\n",
      "20/07/30 00:12:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@391b0d27{/stages/stage,null,AVAILABLE}\n",
      "20/07/30 00:12:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@55d1c131{/stages/stage/json,null,AVAILABLE}\n",
      "20/07/30 00:12:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4c770b28{/stages/pool,null,AVAILABLE}\n",
      "20/07/30 00:12:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@39199d4e{/stages/pool/json,null,AVAILABLE}\n",
      "20/07/30 00:12:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@588acd7f{/storage,null,AVAILABLE}\n",
      "20/07/30 00:12:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3095716{/storage/json,null,AVAILABLE}\n",
      "20/07/30 00:12:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@77d33214{/storage/rdd,null,AVAILABLE}\n",
      "20/07/30 00:12:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@255324d{/storage/rdd/json,null,AVAILABLE}\n",
      "20/07/30 00:12:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@cbaf64d{/environment,null,AVAILABLE}\n",
      "20/07/30 00:12:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4542b12b{/environment/json,null,AVAILABLE}\n",
      "20/07/30 00:12:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2b6f7805{/executors,null,AVAILABLE}\n",
      "20/07/30 00:12:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@83f07f{/executors/json,null,AVAILABLE}\n",
      "20/07/30 00:12:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5bd076bc{/executors/threadDump,null,AVAILABLE}\n",
      "20/07/30 00:12:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@bcb4f04{/executors/threadDump/json,null,AVAILABLE}\n",
      "20/07/30 00:12:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5bb4b128{/static,null,AVAILABLE}\n",
      "20/07/30 00:12:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@442ba229{/,null,AVAILABLE}\n",
      "20/07/30 00:12:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1c165986{/api,null,AVAILABLE}\n",
      "20/07/30 00:12:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4df4866f{/stages/stage/kill,null,AVAILABLE}\n",
      "20/07/30 00:12:29 INFO ServerConnector: Started ServerConnector@5fb145c1{HTTP/1.1}{0.0.0.0:4040}\n",
      "20/07/30 00:12:29 INFO Server: Started @3017ms\n",
      "20/07/30 00:12:29 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "20/07/30 00:12:29 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.51:4040\n",
      "20/07/30 00:12:29 INFO SparkContext: Added file file:/home/dcdsdepeizhang/demo/spark-mllib.py at file:/home/dcdsdepeizhang/demo/spark-mllib.py with timestamp 1596093149878\n",
      "20/07/30 00:12:29 INFO Utils: Copying /home/dcdsdepeizhang/demo/spark-mllib.py to /tmp/spark-ea334f34-2a42-4ad9-86b2-b878fc72ac36/userFiles-03b57ffa-1561-43f4-97ef-04581d73978a/spark-mllib.py\n",
      "20/07/30 00:12:30 INFO Executor: Starting executor ID driver on host localhost\n",
      "20/07/30 00:12:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36616.\n",
      "20/07/30 00:12:30 INFO NettyBlockTransferService: Server created on 192.168.1.51:36616\n",
      "20/07/30 00:12:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.51, 36616)\n",
      "20/07/30 00:12:30 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.51:36616 with 366.3 MB RAM, BlockManagerId(driver, 192.168.1.51, 36616)\n",
      "20/07/30 00:12:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.51, 36616)\n",
      "20/07/30 00:12:30 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6ac7d78c{/metrics/json,null,AVAILABLE}\n",
      "20/07/30 00:12:31 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/local-1596093149967\n",
      "\n",
      "Create DataFrame\n",
      "\n",
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- SepalLengthCm: double (nullable = true)\n",
      " |-- SepalWidthCm: double (nullable = true)\n",
      " |-- PetalLengthCm: double (nullable = true)\n",
      " |-- PetalWidthCm: double (nullable = true)\n",
      " |-- Species: string (nullable = true)\n",
      "\n",
      "+-------+------------------+------------------+-------------------+------------------+------------------+\n",
      "|summary|                Id|     SepalLengthCm|       SepalWidthCm|     PetalLengthCm|      PetalWidthCm|\n",
      "+-------+------------------+------------------+-------------------+------------------+------------------+\n",
      "|  count|               150|               150|                150|               150|               150|\n",
      "|   mean|              75.5| 5.843333333333335| 3.0540000000000007|3.7586666666666693|1.1986666666666672|\n",
      "| stddev|43.445367992456916|0.8280661279778637|0.43359431136217375| 1.764420419952262|0.7631607417008414|\n",
      "|    min|                 1|               4.3|                2.0|               1.0|               0.1|\n",
      "|    max|               150|               7.9|                4.4|               6.9|               2.5|\n",
      "+-------+------------------+------------------+-------------------+------------------+------------------+\n",
      "\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Vectorize Features & Label Target\n",
      "\n",
      "+---+-------------+------------+-------------+------------+-----------+-----------------+-----+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|         features|label|\n",
      "+---+-------------+------------+-------------+------------+-----------+-----------------+-----+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|  0.0|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|[4.9,3.0,1.4,0.2]|  0.0|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|[4.7,3.2,1.3,0.2]|  0.0|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|[4.6,3.1,1.5,0.2]|  0.0|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|[5.0,3.6,1.4,0.2]|  0.0|\n",
      "+---+-------------+------------+-------------+------------+-----------+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Naive Bayes Model\n",
      "\n",
      "NaiveBayes_41a48a0843d4e974398f\n",
      "\n",
      "Test Model on a Trial Vector\n",
      "\n",
      "+-----------------+--------------------+--------------------+----------+\n",
      "|         features|       rawPrediction|         probability|prediction|\n",
      "+-----------------+--------------------+--------------------+----------+\n",
      "|[5.1,3.5,1.4,0.2]|[-12.009647606276...|[0.73942917728046...|       0.0|\n",
      "+-----------------+--------------------+--------------------+----------+\n",
      "\n",
      "\n",
      "Predict on Test Set\n",
      "\n",
      "+-----------------+-----+--------------------+--------------------+----------+\n",
      "|         features|label|       rawPrediction|         probability|prediction|\n",
      "+-----------------+-----+--------------------+--------------------+----------+\n",
      "|[4.7,3.2,1.3,0.2]|  0.0|[-11.206303389856...|[0.70261543063230...|       0.0|\n",
      "|[4.6,3.4,1.4,0.3]|  0.0|[-11.909458686719...|[0.67840435920928...|       0.0|\n",
      "|[5.7,3.8,1.7,0.3]|  0.0|[-13.705049870485...|[0.72699670455887...|       0.0|\n",
      "|[5.1,3.8,1.5,0.3]|  0.0|[-12.892543444085...|[0.73286501768184...|       0.0|\n",
      "|[5.0,3.4,1.6,0.4]|  0.0|[-12.943647354122...|[0.63182767591498...|       0.0|\n",
      "+-----------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Evaluate Model\n",
      "\n",
      "0.944444444444\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "\n",
      "Export Model & Import Model\n",
      "\n",
      "+-----------------+--------------------+--------------------+----------+\n",
      "|         features|       rawPrediction|         probability|prediction|\n",
      "+-----------------+--------------------+--------------------+----------+\n",
      "|[5.1,3.5,1.4,0.2]|[-12.009647606276...|[0.73942917728046...|       0.0|\n",
      "+-----------------+--------------------+--------------------+----------+\n",
      "\n",
      "\n",
      "Decision Tree Model\n",
      "\n",
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_411b8127a2e9a6a9c42a) of depth 5 with 15 nodes\n",
      "  If (feature 2 <= 1.9)\n",
      "   Predict: 0.0\n",
      "  Else (feature 2 > 1.9)\n",
      "   If (feature 3 <= 1.7)\n",
      "    If (feature 2 <= 4.9)\n",
      "     Predict: 1.0\n",
      "    Else (feature 2 > 4.9)\n",
      "     If (feature 3 <= 1.5)\n",
      "      Predict: 2.0\n",
      "     Else (feature 3 > 1.5)\n",
      "      If (feature 0 <= 6.7)\n",
      "       Predict: 1.0\n",
      "      Else (feature 0 > 6.7)\n",
      "       Predict: 2.0\n",
      "   Else (feature 3 > 1.7)\n",
      "    If (feature 2 <= 4.8)\n",
      "     If (feature 0 <= 5.9)\n",
      "      Predict: 1.0\n",
      "     Else (feature 0 > 5.9)\n",
      "      Predict: 2.0\n",
      "    Else (feature 2 > 4.8)\n",
      "     Predict: 2.0\n",
      "\n",
      "\n",
      "Test Model on a Trial Vector\n",
      "\n",
      "+-----------------+--------------+-------------+----------+\n",
      "|         features| rawPrediction|  probability|prediction|\n",
      "+-----------------+--------------+-------------+----------+\n",
      "|[5.1,3.5,1.4,0.2]|[36.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "+-----------------+--------------+-------------+----------+\n",
      "\n",
      "\n",
      "Predict on Test Set\n",
      "\n",
      "+-----------------+-----+--------------+-------------+----------+\n",
      "|         features|label| rawPrediction|  probability|prediction|\n",
      "+-----------------+-----+--------------+-------------+----------+\n",
      "|[4.7,3.2,1.3,0.2]|  0.0|[36.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|[4.6,3.4,1.4,0.3]|  0.0|[36.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|[5.7,3.8,1.7,0.3]|  0.0|[36.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|[5.1,3.8,1.5,0.3]|  0.0|[36.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|[5.0,3.4,1.6,0.4]|  0.0|[36.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "+-----------------+-----+--------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Evaluate Model\n",
      "\n",
      "0.972279733149\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    }
   ],
   "source": [
    "SPARK_MAJOR_VERSION=2 spark-submit spark-mllib.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ssh] Closing existing connection.\n",
      "[ssh] Successfully logged out.\n"
     ]
    }
   ],
   "source": [
    "%logout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSH",
   "language": "bash",
   "name": "ssh"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "ssh"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
