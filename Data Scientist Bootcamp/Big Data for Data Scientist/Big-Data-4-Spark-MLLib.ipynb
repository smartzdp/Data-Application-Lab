{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data 4 - Spark MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ssh] Login to montana.dataapplab.com...\n",
      "[ssh] host=montana.dataapplab.com hostname=montana.dataapplab.com other_conf={'user': 'dcdsdepeizhang', 'port': 49233, 'keyfile': ['/home/dz3vg/.ssh/id_rsa'], 'load_system_ssh_config': False, 'missing_host_policy': <paramiko.client.WarningPolicy object at 0x7fdb0e2c7a50>}\n",
      "[ssh] forwarding local agent\n",
      "[ssh] Successfully logged in.\n"
     ]
    }
   ],
   "source": [
    "%login montana.dataapplab.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ssh] host = montana.dataapplab.com, cwd = /home/dcdsdepeizhang\n",
      "/home/dcdsdepeizhang\n"
     ]
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ssh] host = montana.dataapplab.com, cwd = /home/dcdsdepeizhang\n",
      "data\n",
      "demo\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ssh] host = montana.dataapplab.com, cwd = /home/dcdsdepeizhang\n",
      "[ssh] new cwd: /home/dcdsdepeizhang/demo\n"
     ]
    }
   ],
   "source": [
    "cd demo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ssh] host = montana.dataapplab.com, cwd = /home/dcdsdepeizhang/demo\n"
     ]
    }
   ],
   "source": [
    "hdfs dfs -rm -r -f hdfs:///user/dcdsdepeizhang/data/iris.csv hdfs:///user/dcdsdepeizhang/models/iris-NaiveBayesModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ssh] host = montana.dataapplab.com, cwd = /home/dcdsdepeizhang/demo\n"
     ]
    }
   ],
   "source": [
    "hdfs dfs -cp hdfs:///user/jason/data/iris.csv hdfs:///user/dcdsdepeizhang/data/iris.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ssh] host = montana.dataapplab.com, cwd = /home/dcdsdepeizhang/demo\n",
      "-rw-r--r--   3 dcdsdepeizhang student       5107 2020-07-30 00:22 hdfs:///user/dcdsdepeizhang/data/iris.csv\n"
     ]
    }
   ],
   "source": [
    "hdfs dfs -ls hdfs:///user/dcdsdepeizhang/data/iris.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ssh] host = montana.dataapplab.com, cwd = /home/dcdsdepeizhang/demo\n"
     ]
    }
   ],
   "source": [
    "cat << EOF > spark-mllib.py\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import NaiveBayes, NaiveBayesModel, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "\n",
    "struct = StructType([ \n",
    "    StructField('Id',            IntegerType(), True), \n",
    "    StructField('SepalLengthCm', DoubleType(),  True), \n",
    "    StructField('SepalWidthCm',  DoubleType(),  True), \n",
    "    StructField('PetalLengthCm', DoubleType(),  True), \n",
    "    StructField('PetalWidthCm',  DoubleType(),  True), \n",
    "    StructField('Species',       StringType(),  True)\n",
    "])\n",
    "\n",
    "print \"\\nCreate DataFrame\\n\"\n",
    "df_iris = spark.read.csv('hdfs:///user/dcdsdepeizhang/data/iris.csv', header=True, schema=struct)\n",
    "df_iris.printSchema()\n",
    "df_iris.describe().show()\n",
    "df_iris.show(5)\n",
    "\n",
    "print \"\\nVectorize Features & Label Target\\n\"\n",
    "vecAssembler = VectorAssembler(inputCols=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"], outputCol=\"features\")\n",
    "df_features = vecAssembler.transform(df_iris)\n",
    "stringIndexer = StringIndexer(inputCol=\"Species\", outputCol=\"label\")\n",
    "df_features_label = stringIndexer.fit(df_features).transform(df_features)\n",
    "df_features_label.show(5)\n",
    "# df_features_label.select('label').distinct().show()\n",
    "\n",
    "print \"\\nNaive Bayes Model\\n\"\n",
    "df_train, df_test = df_features_label.randomSplit([.7, .3])\n",
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\")\n",
    "nb_model = nb.fit(df_train)\n",
    "print nb_model\n",
    "\n",
    "print \"\\nTest Model on a Trial Vector\\n\"\n",
    "X_trial = sc.parallelize([Row(features=Vectors.dense([5.1, 3.5, 1.4, 0.2]))]).toDF()\n",
    "y_trial = nb_model.transform(X_trial)\n",
    "y_trial.show()\n",
    "\n",
    "print \"\\nPredict on Test Set\\n\"\n",
    "df_predicted = nb_model.transform(df_test.select('features', 'label'))\n",
    "df_predicted.show(5)\n",
    "\n",
    "print \"\\nEvaluate Model\\n\"\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "print evaluator.evaluate(df_predicted)\n",
    "\n",
    "print \"\\nExport Model & Import Model\\n\"\n",
    "output_dir = \"hdfs:///user/dcdsdepeizhang/models/iris-NaiveBayesModel\"\n",
    "nb_model.save(output_dir)\n",
    "nb_model = NaiveBayesModel.load(output_dir)\n",
    "y_trial = nb_model.transform(X_trial)\n",
    "y_trial.show()\n",
    "\n",
    "print \"\\nDecision Tree Model\\n\"\n",
    "dt = DecisionTreeClassifier(maxDepth=5, featuresCol=\"features\", labelCol=\"label\")\n",
    "dt_model = dt.fit(df_train)\n",
    "print dt_model.toDebugString\n",
    "\n",
    "print \"\\nTest Model on a Trial Vector\\n\"\n",
    "y_trial = dt_model.transform(X_trial)\n",
    "y_trial.show()\n",
    "\n",
    "print \"\\nPredict on Test Set\\n\"\n",
    "df_predicted = dt_model.transform(df_test.select('features', 'label'))\n",
    "df_predicted.show(5)\n",
    "\n",
    "print \"\\nEvaluate Model\\n\"\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "print evaluator.evaluate(df_predicted)\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ssh] host = montana.dataapplab.com, cwd = /home/dcdsdepeizhang/demo\n",
      "SPARK_MAJOR_VERSION is set to 2, using Spark2\n",
      "20/07/30 00:22:52 INFO SparkContext: Running Spark version 2.0.0.2.5.3.0-37\n",
      "20/07/30 00:22:53 INFO SecurityManager: Changing view acls to: dcdsdepeizhang\n",
      "20/07/30 00:22:53 INFO SecurityManager: Changing modify acls to: dcdsdepeizhang\n",
      "20/07/30 00:22:53 INFO SecurityManager: Changing view acls groups to: \n",
      "20/07/30 00:22:53 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/07/30 00:22:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dcdsdepeizhang); groups with view permissions: Set(); users  with modify permissions: Set(dcdsdepeizhang); groups with modify permissions: Set()\n",
      "20/07/30 00:22:53 INFO Utils: Successfully started service 'sparkDriver' on port 41324.\n",
      "20/07/30 00:22:53 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/07/30 00:22:53 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/07/30 00:22:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f459e15c-3e6d-4e2f-bb51-2683f7ef9eb1\n",
      "20/07/30 00:22:53 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "20/07/30 00:22:53 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/07/30 00:22:54 INFO log: Logging initialized @2673ms\n",
      "20/07/30 00:22:54 INFO Server: jetty-9.2.z-SNAPSHOT\n",
      "20/07/30 00:22:54 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@12c15a9f{/jobs,null,AVAILABLE}\n",
      "20/07/30 00:22:54 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1da7dfaf{/jobs/json,null,AVAILABLE}\n",
      "20/07/30 00:22:54 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@17571bff{/jobs/job,null,AVAILABLE}\n",
      "20/07/30 00:22:54 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2d482dd8{/jobs/job/json,null,AVAILABLE}\n",
      "20/07/30 00:22:54 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@35458f4c{/stages,null,AVAILABLE}\n",
      "20/07/30 00:22:54 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@16bc64ec{/stages/json,null,AVAILABLE}\n",
      "20/07/30 00:22:54 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@391b0d27{/stages/stage,null,AVAILABLE}\n",
      "20/07/30 00:22:54 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@55d1c131{/stages/stage/json,null,AVAILABLE}\n",
      "20/07/30 00:22:54 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4c770b28{/stages/pool,null,AVAILABLE}\n",
      "20/07/30 00:22:54 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@39199d4e{/stages/pool/json,null,AVAILABLE}\n",
      "20/07/30 00:22:54 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@588acd7f{/storage,null,AVAILABLE}\n",
      "20/07/30 00:22:54 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3095716{/storage/json,null,AVAILABLE}\n",
      "20/07/30 00:22:54 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@77d33214{/storage/rdd,null,AVAILABLE}\n",
      "20/07/30 00:22:54 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@255324d{/storage/rdd/json,null,AVAILABLE}\n",
      "20/07/30 00:22:54 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@cbaf64d{/environment,null,AVAILABLE}\n",
      "20/07/30 00:22:54 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4542b12b{/environment/json,null,AVAILABLE}\n",
      "20/07/30 00:22:54 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2b6f7805{/executors,null,AVAILABLE}\n",
      "20/07/30 00:22:54 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@83f07f{/executors/json,null,AVAILABLE}\n",
      "20/07/30 00:22:54 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5bd076bc{/executors/threadDump,null,AVAILABLE}\n",
      "20/07/30 00:22:54 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@bcb4f04{/executors/threadDump/json,null,AVAILABLE}\n",
      "20/07/30 00:22:54 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5bb4b128{/static,null,AVAILABLE}\n",
      "20/07/30 00:22:54 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@442ba229{/,null,AVAILABLE}\n",
      "20/07/30 00:22:54 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1c165986{/api,null,AVAILABLE}\n",
      "20/07/30 00:22:54 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4df4866f{/stages/stage/kill,null,AVAILABLE}\n",
      "20/07/30 00:22:54 INFO ServerConnector: Started ServerConnector@6e72a715{HTTP/1.1}{0.0.0.0:4040}\n",
      "20/07/30 00:22:54 INFO Server: Started @2943ms\n",
      "20/07/30 00:22:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "20/07/30 00:22:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.51:4040\n",
      "20/07/30 00:22:54 INFO SparkContext: Added file file:/home/dcdsdepeizhang/demo/spark-mllib.py at file:/home/dcdsdepeizhang/demo/spark-mllib.py with timestamp 1596093774683\n",
      "20/07/30 00:22:54 INFO Utils: Copying /home/dcdsdepeizhang/demo/spark-mllib.py to /tmp/spark-960ccb86-8d33-43cf-baeb-2840d38a2338/userFiles-0a7ca8fc-0e54-4ed3-abbb-93f3660c7d2b/spark-mllib.py\n",
      "20/07/30 00:22:54 INFO Executor: Starting executor ID driver on host localhost\n",
      "20/07/30 00:22:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36767.\n",
      "20/07/30 00:22:54 INFO NettyBlockTransferService: Server created on 192.168.1.51:36767\n",
      "20/07/30 00:22:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.51, 36767)\n",
      "20/07/30 00:22:54 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.51:36767 with 366.3 MB RAM, BlockManagerId(driver, 192.168.1.51, 36767)\n",
      "20/07/30 00:22:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.51, 36767)\n",
      "20/07/30 00:22:55 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@46b8e385{/metrics/json,null,AVAILABLE}\n",
      "20/07/30 00:22:55 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/local-1596093774755\n",
      "\n",
      "Create DataFrame\n",
      "\n",
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- SepalLengthCm: double (nullable = true)\n",
      " |-- SepalWidthCm: double (nullable = true)\n",
      " |-- PetalLengthCm: double (nullable = true)\n",
      " |-- PetalWidthCm: double (nullable = true)\n",
      " |-- Species: string (nullable = true)\n",
      "\n",
      "+-------+------------------+------------------+-------------------+------------------+------------------+\n",
      "|summary|                Id|     SepalLengthCm|       SepalWidthCm|     PetalLengthCm|      PetalWidthCm|\n",
      "+-------+------------------+------------------+-------------------+------------------+------------------+\n",
      "|  count|               150|               150|                150|               150|               150|\n",
      "|   mean|              75.5| 5.843333333333335| 3.0540000000000007|3.7586666666666693|1.1986666666666672|\n",
      "| stddev|43.445367992456916|0.8280661279778637|0.43359431136217375| 1.764420419952262|0.7631607417008414|\n",
      "|    min|                 1|               4.3|                2.0|               1.0|               0.1|\n",
      "|    max|               150|               7.9|                4.4|               6.9|               2.5|\n",
      "+-------+------------------+------------------+-------------------+------------------+------------------+\n",
      "\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Vectorize Features & Label Target\n",
      "\n",
      "+---+-------------+------------+-------------+------------+-----------+-----------------+-----+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|         features|label|\n",
      "+---+-------------+------------+-------------+------------+-----------+-----------------+-----+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|  0.0|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|[4.9,3.0,1.4,0.2]|  0.0|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|[4.7,3.2,1.3,0.2]|  0.0|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|[4.6,3.1,1.5,0.2]|  0.0|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|[5.0,3.6,1.4,0.2]|  0.0|\n",
      "+---+-------------+------------+-------------+------------+-----------+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Naive Bayes Model\n",
      "\n",
      "NaiveBayes_4a59b5398d5c9eedf112\n",
      "\n",
      "Test Model on a Trial Vector\n",
      "\n",
      "+-----------------+--------------------+--------------------+----------+\n",
      "|         features|       rawPrediction|         probability|prediction|\n",
      "+-----------------+--------------------+--------------------+----------+\n",
      "|[5.1,3.5,1.4,0.2]|[-12.050964396205...|[0.72276394448243...|       0.0|\n",
      "+-----------------+--------------------+--------------------+----------+\n",
      "\n",
      "\n",
      "Predict on Test Set\n",
      "\n",
      "+-----------------+-----+--------------------+--------------------+----------+\n",
      "|         features|label|       rawPrediction|         probability|prediction|\n",
      "+-----------------+-----+--------------------+--------------------+----------+\n",
      "|[4.7,3.2,1.3,0.2]|  0.0|[-11.246218093207...|[0.68553736108198...|       0.0|\n",
      "|[5.0,3.6,1.4,0.2]|  0.0|[-12.089663132240...|[0.73045953831294...|       0.0|\n",
      "|[5.4,3.9,1.7,0.4]|  0.0|[-13.997453295561...|[0.68311901725000...|       0.0|\n",
      "|[4.8,3.0,1.4,0.1]|  0.0|[-10.933804966399...|[0.67608956099912...|       0.0|\n",
      "|[5.7,3.8,1.7,0.3]|  0.0|[-13.742284752804...|[0.71036392964820...|       0.0|\n",
      "+-----------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Evaluate Model\n",
      "\n",
      "0.979116885703\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "\n",
      "Export Model & Import Model\n",
      "\n",
      "+-----------------+--------------------+--------------------+----------+\n",
      "|         features|       rawPrediction|         probability|prediction|\n",
      "+-----------------+--------------------+--------------------+----------+\n",
      "|[5.1,3.5,1.4,0.2]|[-12.050964396205...|[0.72276394448243...|       0.0|\n",
      "+-----------------+--------------------+--------------------+----------+\n",
      "\n",
      "\n",
      "Decision Tree Model\n",
      "\n",
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_49a9ade1a2fd3a746aaf) of depth 3 with 9 nodes\n",
      "  If (feature 2 <= 4.8)\n",
      "   If (feature 2 <= 1.7)\n",
      "    Predict: 0.0\n",
      "   Else (feature 2 > 1.7)\n",
      "    Predict: 1.0\n",
      "  Else (feature 2 > 4.8)\n",
      "   If (feature 3 <= 1.5)\n",
      "    If (feature 1 <= 2.5)\n",
      "     Predict: 1.0\n",
      "    Else (feature 1 > 2.5)\n",
      "     Predict: 2.0\n",
      "   Else (feature 3 > 1.5)\n",
      "    Predict: 2.0\n",
      "\n",
      "\n",
      "Test Model on a Trial Vector\n",
      "\n",
      "+-----------------+--------------+-------------+----------+\n",
      "|         features| rawPrediction|  probability|prediction|\n",
      "+-----------------+--------------+-------------+----------+\n",
      "|[5.1,3.5,1.4,0.2]|[31.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "+-----------------+--------------+-------------+----------+\n",
      "\n",
      "\n",
      "Predict on Test Set\n",
      "\n",
      "+-----------------+-----+--------------+-------------+----------+\n",
      "|         features|label| rawPrediction|  probability|prediction|\n",
      "+-----------------+-----+--------------+-------------+----------+\n",
      "|[4.7,3.2,1.3,0.2]|  0.0|[31.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|[5.0,3.6,1.4,0.2]|  0.0|[31.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|[5.4,3.9,1.7,0.4]|  0.0|[31.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|[4.8,3.0,1.4,0.1]|  0.0|[31.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|[5.7,3.8,1.7,0.3]|  0.0|[31.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "+-----------------+-----+--------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Evaluate Model\n",
      "\n",
      "0.817900805422\n"
     ]
    }
   ],
   "source": [
    "SPARK_MAJOR_VERSION=2 spark-submit spark-mllib.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ssh] Closing existing connection.\n",
      "[ssh] Successfully logged out.\n"
     ]
    }
   ],
   "source": [
    "%logout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSH",
   "language": "bash",
   "name": "ssh"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "ssh"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
